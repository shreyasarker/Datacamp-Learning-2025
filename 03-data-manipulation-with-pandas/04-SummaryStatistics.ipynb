{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57513799",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "### What Are Summary Statistics?\n",
    "\n",
    "Summary statistics are key figures that give us a quick snapshot of a dataset. Instead of looking at every single value, we can use summary statistics to understand **patterns**, **central tendencies**, **variability**, and **distribution** of data.\n",
    "\n",
    "These statistics help answer questions like:\n",
    "- What’s the **average** number of homeless individuals per state?\n",
    "- What’s the **maximum** or **minimum** number of family members reported?\n",
    "- How **spread out** are the state populations?\n",
    "\n",
    "### Common Types of Summary Statistics\n",
    "\n",
    "Here are some of the most frequently used summary metrics in data analysis:\n",
    "\n",
    "#### 1. **Mean (Average)**\n",
    "- Tells us the central value of a numerical column.\n",
    "- _Example_: The average number of homeless individuals across all states.\n",
    "\n",
    "#### 2. **Median**\n",
    "- The middle value when all data is sorted.\n",
    "- Useful when data is skewed.\n",
    "- _Example_: Median population per state to understand typical state size without influence from very large or small states.\n",
    "\n",
    "#### 3. **Maximum and Minimum**\n",
    "- Highest and lowest values in a column.\n",
    "- _Example_: Which state has the **maximum** number of homeless individuals? Which one has the **minimum**?\n",
    "\n",
    "#### 4. **Standard Deviation**\n",
    "- Shows how much the values vary from the mean.\n",
    "- _Example_: How consistent is the number of family members across all states?\n",
    "\n",
    "#### 5. **Count**\n",
    "- Total number of non-missing (valid) entries.\n",
    "- _Example_: How many states have data recorded for individual homelessness?\n",
    "\n",
    "### Why Use Summary Statistics?\n",
    "\n",
    "- **Quick insights** without looking at raw data.\n",
    "- Helps in **data cleaning** by identifying outliers or missing values.\n",
    "- Crucial for **exploratory data analysis (EDA)** before deeper modeling or visualization.\n",
    "- Sets the foundation for more complex techniques like machine learning.\n",
    "\n",
    "### Example Use Cases\n",
    "\n",
    "- You might **compare the average homelessness rates** between different regions of the USA.\n",
    "- You could **track the range** (difference between max and min) of state populations.\n",
    "- Summary stats help determine if data is **skewed**, which affects how we interpret results and choose models.\n",
    "\n",
    "Summary statistics are the **first step** in any data exploration. They provide clarity and direction before diving into deeper analysis or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262596de",
   "metadata": {},
   "source": [
    "## Exercise: Mean and median\n",
    "\n",
    "Summary statistics help us get a quick overview of our dataset by calculating values like the mean, median, minimum, maximum, and standard deviation. These values give us insight into the structure and distribution of our data — especially useful when dealing with large datasets.\n",
    "\n",
    "In this task, we’ll get familiar with a DataFrame called `sales`, which contains weekly sales data.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. View the first few entries of the dataset to understand its structure.\n",
    "2. Get a summary of column names, data types, and non-null counts.\n",
    "3. Calculate the **mean** of the `weekly_sales` column to understand the average sales.\n",
    "4. Calculate the **median** of the `weekly_sales` column to see the midpoint of the sales distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfe89f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  store type  department        date  weekly_sales  is_holiday  \\\n",
      "0           0      1    A           1  2010-02-05      24924.50       False   \n",
      "1           1      1    A           1  2010-03-05      21827.90       False   \n",
      "2           2      1    A           1  2010-04-02      57258.43       False   \n",
      "3           3      1    A           1  2010-05-07      17413.94       False   \n",
      "4           4      1    A           1  2010-06-04      17558.09       False   \n",
      "\n",
      "   temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0       5.727778              0.679451         8.106  \n",
      "1       8.055556              0.693452         8.106  \n",
      "2      16.816667              0.718284         7.808  \n",
      "3      22.527778              0.748928         7.808  \n",
      "4      27.050000              0.714586         7.808  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sales = pd.read_csv(\"datasets/sales_subset.csv\")\n",
    "# Preview the first five rows of the sales DataFrame\n",
    "print(sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc42056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10774 entries, 0 to 10773\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Unnamed: 0            10774 non-null  int64  \n",
      " 1   store                 10774 non-null  int64  \n",
      " 2   type                  10774 non-null  object \n",
      " 3   department            10774 non-null  int64  \n",
      " 4   date                  10774 non-null  object \n",
      " 5   weekly_sales          10774 non-null  float64\n",
      " 6   is_holiday            10774 non-null  bool   \n",
      " 7   temperature_c         10774 non-null  float64\n",
      " 8   fuel_price_usd_per_l  10774 non-null  float64\n",
      " 9   unemployment          10774 non-null  float64\n",
      "dtypes: bool(1), float64(4), int64(3), object(2)\n",
      "memory usage: 768.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display detailed info about the sales DataFrame\n",
    "print(sales.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4f4240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean weekly sales: 23843.95014850566\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display the average weekly sales\n",
    "average_sales = sales[\"weekly_sales\"].mean()\n",
    "print(\"Mean weekly sales:\", average_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e48513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median weekly sales: 12049.064999999999\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display the median weekly sales\n",
    "median_sales = sales[\"weekly_sales\"].median()\n",
    "print(\"Median weekly sales:\", median_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71860153",
   "metadata": {},
   "source": [
    "## Exercise: Summarizing dates\n",
    "\n",
    "### Overview\n",
    "\n",
    "Date columns in a dataset can also be summarized using specific statistical functions. While calculating something like a **mean** on dates might not be very meaningful, identifying the **earliest** and **latest** dates can be incredibly helpful to understand the **time range** your dataset spans.\n",
    "\n",
    "In this exercise, we’ll focus on summarizing the date column in the `sales` DataFrame.\n",
    "\n",
    "The dataset `sales` is already available, and pandas has been imported as `pd`.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Find and print the **latest date** in the dataset.\n",
    "2. Find and print the **earliest date** in the dataset.\n",
    "\n",
    "These steps will help you determine the range of time your sales data covers.\n",
    "\n",
    "###  Example\n",
    "\n",
    "If the latest date is `2020-12-31` and the earliest date is `2018-01-01`, it tells you that the data spans three full years.\n",
    "\n",
    "> Tip: This is especially useful for validating time-series data or preparing to group data by time intervals like months or years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e60fb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest date in dataset: 2012-10-26\n",
      "Earliest date in dataset: 2010-02-05\n"
     ]
    }
   ],
   "source": [
    "# Display the most recent date in the sales data\n",
    "latest_date = sales[\"date\"].max()\n",
    "print(\"Latest date in dataset:\", latest_date)\n",
    "\n",
    "# Display the earliest date in the sales data\n",
    "earliest_date = sales[\"date\"].min()\n",
    "print(\"Earliest date in dataset:\", earliest_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04778280",
   "metadata": {},
   "source": [
    "## Exercise: Efficient Summaries\n",
    "\n",
    "In data analysis, we often need to summarize columns beyond just the basic statistics like mean or standard deviation. Sometimes, we need **custom summaries** — for example, to better understand distributions that contain **outliers**.\n",
    "\n",
    "The `.agg()` method in pandas allows us to:\n",
    "\n",
    "* Apply **custom functions** to columns.\n",
    "* Apply **multiple functions** across **multiple columns** in a single, efficient operation.\n",
    "\n",
    "A useful example is the **interquartile range (IQR)** — the difference between the 75th and 25th percentiles. It's a robust measure of spread that isn't easily affected by extreme values.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Use a custom `iqr()` function to calculate the IQR of the `temperature_c` column using `.agg()`.\n",
    "2. Expand the aggregation to also include the `fuel_price_usd_per_l` and `unemployment` columns, still using the `iqr()` function.\n",
    "3. Enhance the summary by applying **both** `iqr` and `median` (from NumPy) to all three columns at once using `.agg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6531c58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR of temperature (°C): 16.583333333333336\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate the interquartile range (IQR)\n",
    "def iqr(series):\n",
    "    return series.quantile(0.75) - series.quantile(0.25)\n",
    "\n",
    "# Display the IQR of the 'temperature_c' column\n",
    "temperature_iqr = sales['temperature_c'].agg(iqr)\n",
    "print(\"IQR of temperature (°C):\", temperature_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f26a8e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interquartile Ranges (IQR):\n",
      "temperature_c           16.583333\n",
      "fuel_price_usd_per_l     0.073176\n",
      "unemployment             0.565000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate the interquartile range (IQR)\n",
    "def iqr(series):\n",
    "    return series.quantile(0.75) - series.quantile(0.25)\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = [\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]\n",
    "\n",
    "# Calculate and display the IQR for the selected columns\n",
    "iqr_values = sales[selected_columns].agg(iqr)\n",
    "print(\"Interquartile Ranges (IQR):\")\n",
    "print(iqr_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f031ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics (IQR and Median):\n",
      "             temperature_c  fuel_price_usd_per_l  unemployment\n",
      "iqr              16.583333              0.073176         0.565\n",
      "median_func      16.966667              0.743381         8.099\n"
     ]
    }
   ],
   "source": [
    "# Import NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Define custom function to calculate IQR (Interquartile Range)\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Wrap np.median in a custom function\n",
    "def median_func(column):\n",
    "    return np.median(column.values)\n",
    "\n",
    "# Select the numeric columns of interest\n",
    "columns_to_summarize = [\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]\n",
    "\n",
    "# Apply both IQR and median using agg()\n",
    "summary_stats = sales[columns_to_summarize].agg([iqr, median_func])\n",
    "\n",
    "# Print the resulting summary statistics\n",
    "print(\"Summary Statistics (IQR and Median):\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d973d39",
   "metadata": {},
   "source": [
    "## Exercise: Cumulative Statistics\n",
    "\n",
    "Cumulative statistics are useful when you want to observe how metrics build up over time. Instead of looking at just a single week's performance, you can track ongoing totals and record-breaking values. In this task, you'll be calculating the **cumulative total sales** and the **highest sales recorded so far** for a department over time.\n",
    "\n",
    "You're given a DataFrame named `sales_1_1`, which contains weekly sales data for **department 1** of **store 1**. Your goal is to track how total sales and maximum sales evolve across the dates.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. **Sort** the DataFrame by the `date` column in ascending order to ensure chronological tracking.\n",
    "2. Calculate the **cumulative sum** of `weekly_sales` and store it in a new column named `cum_weekly_sales`. This shows the total sales so far each week.\n",
    "3. Calculate the **cumulative maximum** of `weekly_sales` and store it in a column named `cum_max_sales`. This helps track the highest weekly sales achieved so far.\n",
    "4. Finally, **display** the following columns: `date`, `weekly_sales`, `cum_weekly_sales`, and `cum_max_sales`.\n",
    "\n",
    "These steps give insight into how sales have built up over time and when new sales records were set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b6c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date  weekly_sales  cum_weekly_sales  cum_max_sales\n",
      "0   2010-02-05      24924.50          24924.50       24924.50\n",
      "1   2010-03-05      21827.90          46752.40       24924.50\n",
      "2   2010-04-02      57258.43         104010.83       57258.43\n",
      "3   2010-05-07      17413.94         121424.77       57258.43\n",
      "4   2010-06-04      17558.09         138982.86       57258.43\n",
      "5   2010-07-02      16333.14         155316.00       57258.43\n",
      "6   2010-08-06      17508.41         172824.41       57258.43\n",
      "7   2010-09-03      16241.78         189066.19       57258.43\n",
      "8   2010-10-01      20094.19         209160.38       57258.43\n",
      "9   2010-11-05      34238.88         243399.26       57258.43\n",
      "10  2010-12-03      22517.56         265916.82       57258.43\n",
      "11  2011-01-07      15984.24         281901.06       57258.43\n"
     ]
    }
   ],
   "source": [
    "# Create sales_1_1 which contains weekly sales data for **department 1** of **store 1**\n",
    "sales_1_1 = sales[np.logical_and(sales['department'] == 1, sales['store'] == 1)]\n",
    "\n",
    "# Sort sales_1_1 by date\n",
    "sales_1_1 = sales_1_1.sort_values('date')\n",
    "\n",
    "# Get the cumulative sum of weekly_sales and add it as cum_weekly_sales column\n",
    "sales_1_1['cum_weekly_sales'] = sales_1_1['weekly_sales'].cumsum()\n",
    "\n",
    "# Get the cumulative max of weekly_sales and add it as cum_max_sales column\n",
    "sales_1_1['cum_max_sales'] = sales_1_1['weekly_sales'].cummax()\n",
    "\n",
    "# Display the calculated columns\n",
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
